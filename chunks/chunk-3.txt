================================================================================
CHUNK 4 OF 5
================================================================================


========================================
File: ./tests/data/test_enhanced.py
========================================

import unittest
import os
import tempfile
import shutil
from src.enhanced_chunker import EnhancedParallelChunker

class TestEnhancedParallelChunker(unittest.TestCase):
    def setUp(self):
        self.test_dir = tempfile.mkdtemp()
        
        self.python_file = os.path.join(self.test_dir, "example.py")
        with open(self.python_file, "w", encoding="utf-8") as f:
            f.write('''"""
                This is a test docstring.
                """
                import pandas as pd
                from datetime import datetime

                class TestClass:
                    def __init__(self):
                        pass
                        
                    def test_method(self):
                        # This is a comment
                        pass

                def standalone_function():
                    """Function docstring"""
                    return True
                ''')
        
        self.redundant_file = os.path.join(self.test_dir, "redundant.py")
        with open(self.redundant_file, "w", encoding="utf-8") as f:
            f.write('''def standalone_function():
                """Function docstring"""
                return True
            ''')
        
        self.output_dir = os.path.join(self.test_dir, "output")
        os.makedirs(self.output_dir)

    def tearDown(self):
        shutil.rmtree(self.test_dir)

    def test_metadata_extraction(self):
        """Test if metadata is correctly extracted from files"""
        chunker = EnhancedParallelChunker(
            equal_chunks=1,
            output_dir=self.output_dir,
            extract_metadata=True
        )
        chunker.process_directory(self.test_dir)
        
        chunk_files = [f for f in os.listdir(self.output_dir) if f.startswith('chunk-')]
        self.assertEqual(len(chunk_files), 1)
        
        with open(os.path.join(self.output_dir, chunk_files[0]), 'r') as f:
            content = f.read()
            
        self.assertIn('METADATA:', content)
        self.assertIn('FUNCTIONS: standalone_function, test_method', content)
        self.assertIn('CLASSES: TestClass', content)
        self.assertIn('IMPORTS: import pandas, from datetime', content)
        self.assertIn('This is a test docstring', content)

    def test_relevance_scoring(self):
        """Test if relevance scoring works correctly"""
        chunker = EnhancedParallelChunker(
            equal_chunks=1,
            output_dir=self.output_dir,
            min_relevance_score=0.5  
        )
        
        low_relevance = os.path.join(self.test_dir, "low_relevance.py")
        with open(low_relevance, "w") as f:
            f.write("# Just comments\n# More comments\n# Even more comments\nx = 1")
        
        chunker.process_directory(self.test_dir)
        
        chunk_files = [f for f in os.listdir(self.output_dir) if f.startswith('chunk-')]
        for chunk_file in chunk_files:
            with open(os.path.join(self.output_dir, chunk_file), 'r') as f:
                content = f.read()
                if 'low_relevance.py' in content:
                    relevance_line = [l for l in content.split('\n') if 'RELEVANCE_SCORE:' in l][0]
                    score = float(relevance_line.split(':')[1].strip())
                    self.assertLess(score, 1.0)  

    def test_redundancy_removal(self):
        """Test if redundant content is properly removed"""
        chunker = EnhancedParallelChunker(
            equal_chunks=2,
            output_dir=self.output_dir,
            remove_redundancy=True
        )
        chunker.process_directory(self.test_dir)
        
        all_content = []
        chunk_files = sorted([f for f in os.listdir(self.output_dir) if f.startswith('chunk-')])
        for chunk_file in chunk_files:
            with open(os.path.join(self.output_dir, chunk_file), 'r') as f:
                all_content.append(f.read())
        
        redundant_count = sum(
            content.count('def standalone_function():') 
            for content in all_content
        )
        self.assertEqual(redundant_count, 1, "Redundant function should appear only once")

    def test_context_window_respect(self):
        """Test if chunks respect context window size"""
        small_window = 100
        chunker = EnhancedParallelChunker(
            equal_chunks=2,
            output_dir=self.output_dir,
            context_window=small_window
        )

        for f in os.listdir(self.test_dir):
            print(f"- {f}")
        
        chunker.process_directory(self.test_dir)
        
        chunk_files = [f for f in os.listdir(self.output_dir) if f.startswith('chunk-')]
        
        for chunk_file in chunk_files:
            chunk_path = os.path.join(self.output_dir, chunk_file)
            try:
                with open(chunk_path, 'rb') as f:  
                    content = f.read()
                    size = len(content)
                    if size >= small_window * 1.5:
                        print(f"WARNING: Chunk too large! Size {size} exceeds limit {small_window * 1.5}")
                    self.assertLess(size, small_window * 1.5, 
                        f"Chunk {chunk_file} size {size} exceeds limit {small_window * 1.5}")
            except Exception as e:
                print(f"ERROR reading {chunk_file}: {str(e)}")
                raise

    def test_disable_features(self):
        """Test if features can be properly disabled"""
        chunker = EnhancedParallelChunker(
            equal_chunks=1,
            output_dir=self.output_dir,
            extract_metadata=False,
            add_summaries=False,
            remove_redundancy=False
        )
        chunker.process_directory(self.test_dir)
        
        chunk_files = [f for f in os.listdir(self.output_dir) if f.startswith('chunk-')]
        with open(os.path.join(self.output_dir, chunk_files[0]), 'r') as f:
            content = f.read()
            self.assertNotIn('METADATA:', content)
            self.assertGreater(
                content.count('def standalone_function():'),
                1
            )

    def test_complex_metadata(self):
        """Test metadata extraction from more complex code structures"""
        complex_file = os.path.join(self.test_dir, "complex.py")
        with open(complex_file, "w") as f:
            f.write('''
                from typing import List, Optional
                import os.path as osp
                
                @decorator
                class ComplexClass:
                    """Class docstring"""
                    def __init__(self):
                        pass
                    
                    @property
                    def prop(self): 
                        return None
                        
                    async def async_method(self):
                        pass
            ''')
            
        chunker = EnhancedParallelChunker(equal_chunks=1, output_dir=self.output_dir)
        chunker.process_directory(self.test_dir)
        
        with open(os.path.join(self.output_dir, "chunk-0.txt"), 'r') as f:
            content = f.read()
            self.assertIn('async_method', content)
            self.assertIn('ComplexClass', content)
            self.assertIn('from typing import', content)

    def test_large_file_handling(self):
        """Test handling of large files with context window"""
        large_file = os.path.join(self.test_dir, "large.py")
        with open(large_file, "w") as f:
            f.write("x = 1\n" * 10000)  
            
        chunker = EnhancedParallelChunker(
            equal_chunks=2,
            output_dir=self.output_dir,
            context_window=1000,
            remove_redundancy=True
        )
        chunker.process_directory(self.test_dir)
        
        chunks = [f for f in os.listdir(self.output_dir) if f.startswith('chunk-')]
        for chunk in chunks:
            with open(os.path.join(self.output_dir, chunk), 'r') as f:
                content = f.read()
                self.assertLess(len(content), 1000)

    def test_mixed_content_relevance(self):
        """Test relevance scoring with mixed content types"""
        mixed_file = os.path.join(self.test_dir, "mixed.py")
        with open(mixed_file, "w") as f:
            f.write('''
                # Configuration
                CONFIG = {
                    "key": "value"
                }
                
                def important_function():
                    """Critical business logic"""
                    pass
                    
                # Just some constants
                A = 1
                B = 2
                C = 3
            ''')
            
        chunker = EnhancedParallelChunker(
            equal_chunks=2,
            output_dir=self.output_dir,
            min_relevance_score=0.3
        )
        chunker.process_directory(self.test_dir)
        
        chunks = [f for f in os.listdir(self.output_dir) if f.startswith('chunk-')]
        scores = []
        for chunk in chunks:
            with open(os.path.join(self.output_dir, chunk), 'r') as f:
                content = f.read()
                if 'RELEVANCE_SCORE:' in content:
                    score_line = [l for l in content.split('\n') if 'RELEVANCE_SCORE:' in l][0]
                    scores.append(float(score_line.split(':')[1].strip()))
        
        self.assertTrue(any(s > 0.5 for s in scores), "Should have some high relevance chunks")

if __name__ == '__main__':
    unittest.main()

========================================
File: ./tests/data/test_parallel_chunker.py
========================================

import unittest
import os
import tempfile
import shutil
from src.multi_dirs_chunker import ParallelChunker, PriorityRule

class TestParallelChunker(unittest.TestCase):
    def setUp(self):
        self.test_dir = tempfile.mkdtemp()
        self.sub_dir = os.path.join(self.test_dir, "sub")
        os.mkdir(self.sub_dir)
        self.test_file_1 = os.path.join(self.test_dir, "file1.txt")
        self.test_file_2 = os.path.join(self.sub_dir, "file2.txt")
        self.git_dir = os.path.join(self.test_dir, ".git")
        os.mkdir(self.git_dir)
        self.git_file = os.path.join(self.git_dir, "config")
        self.test_file_bin = os.path.join(self.test_dir, "binary.bin")
        with open(self.test_file_1, "w") as f:
            f.write("This is a test file\nIt has some text.")
        with open(self.test_file_2, "w") as f:
            f.write("Another file\nWith more text.")
        with open(self.git_file, "w") as f:
            f.write("Git config")
        with open(self.test_file_bin, "wb") as f:
            f.write(b"\x00\xff\x10binary")

    def tearDown(self):
        shutil.rmtree(self.test_dir)

    def test_should_ignore_file(self):
        c = ParallelChunker(max_chunk_size=1000) 
        c.current_walk_root = self.test_dir
        self.assertTrue(c.should_ignore_file(self.git_file))
        self.assertFalse(c.should_ignore_file(self.test_file_1))

    def test_is_binary_file(self):
        c = ParallelChunker(max_chunk_size=1000)  
        self.assertTrue(c.is_binary_file(self.test_file_bin))
        self.assertFalse(c.is_binary_file(self.test_file_1))

    def test_process_directory(self):
        c = ParallelChunker(max_chunk_size=1000)
        c.process_directory(self.test_dir)
        self.assertTrue(any("file1.txt" in x[0] for x in c.loaded_files))

    def test_priority_rules(self):
        r = [("*.txt", 10), ("file2*", 20)]
        c = ParallelChunker(priority_rules=r, max_chunk_size=1000) 
        c.process_directory(self.test_dir)
        ps = [p for _, _, p in c.loaded_files]
        self.assertIn(10, ps)
        self.assertIn(20, ps)

    def test_priority_rules_no_match(self):
        r = [("*.md", 50), ("something*", 100)]
        c = ParallelChunker(priority_rules=r, max_chunk_size=1000) 
        c.process_directory(self.test_dir)
        ps = [p for _, _, p in c.loaded_files]
        self.assertTrue(all(x == 0 for x in ps))

    def test_equal_chunks(self):  
        d = os.path.join(self.test_dir, "out")
        os.mkdir(d)
        c = ParallelChunker(equal_chunks=2, output_dir=d)  
        c.process_directory(self.test_dir)
        chunks = [f for f in os.listdir(d) if f.startswith("chunk-")]
        self.assertEqual(len(chunks), 2)

    def test_max_chunk_size(self):  
        d = os.path.join(self.test_dir, "maxout")
        os.mkdir(d)
        c = ParallelChunker(max_chunk_size=5, output_dir=d) 
        c.process_directory(self.test_dir)
        f = [x for x in os.listdir(d) if x.startswith("chunk-")]
        self.assertTrue(len(f) > 1)

    def test_max_chunk_size_empty_file(self): 
        empty_file = os.path.join(self.test_dir, "empty.txt")
        open(empty_file, "w").close()
        d = os.path.join(self.test_dir, "maxout_empty")
        os.mkdir(d)
        c = ParallelChunker(max_chunk_size=5, output_dir=d) 
        c.process_directory(self.test_dir)
        f = [x for x in os.listdir(d) if x.startswith("chunk-")]
        self.assertTrue(len(f) >= 1)

    def test_equal_chunks_exact(self): 
        d = os.path.join(self.test_dir, "equal_chunks")
        os.mkdir(d)
        c = ParallelChunker(
            equal_chunks=2,
            output_dir=d
        )
        c.process_directory(self.test_dir)
        files = [x for x in os.listdir(d) if x.startswith("chunk-")]
        self.assertEqual(len(files), 2)

    def test_user_ignore_patterns(self):
        d = os.path.join(self.test_dir, "ignore_test")
        os.mkdir(d)
        c = ParallelChunker(output_dir=d, user_ignore=["*file2.txt"], max_chunk_size=1000)  
        c.process_directory(self.test_dir)
        loaded = [os.path.basename(x[0]) for x in c.loaded_files]
        self.assertNotIn("file2.txt", loaded)
        self.assertIn("file1.txt", loaded)

    def test_user_unignore_patterns(self):
        d = os.path.join(self.test_dir, "unignore_test")
        os.mkdir(d)
        c = ParallelChunker(output_dir=d, user_ignore=["*.txt"], 
                           user_unignore=["file2.txt"], max_chunk_size=1000)  
        c.process_directory(self.test_dir)
        loaded = [os.path.basename(x[0]) for x in c.loaded_files]
        self.assertIn("file2.txt", loaded)
        self.assertNotIn("file1.txt", loaded)

    def test_no_files_collected(self):
        d = os.path.join(self.test_dir, "empty_dir")
        os.mkdir(d)
        c = ParallelChunker(max_chunk_size=1000)  
        c.process_directory(d)
        self.assertEqual(len(c.loaded_files), 0)

    def test_large_chunk_split(self):
        large_file = os.path.join(self.test_dir, "large.txt")
        with open(large_file, "w") as f:
            f.write("word " * 5000)
        d = os.path.join(self.test_dir, "large_split_out")
        os.mkdir(d)
        c = ParallelChunker(output_dir=d, max_chunk_size=100)  
        c.process_directory(self.test_dir)
        chunk_files = [x for x in os.listdir(d) if x.startswith("chunk-")]
        self.assertTrue(len(chunk_files) > 1)
    
    def test_concurrent_file_loading(self):
        """Test if parallel file loading works correctly with many files"""
        for i in range(100):
            with open(os.path.join(self.test_dir, f"file_{i}.txt"), "w") as f:
                f.write(f"Content {i}")
                
        c = ParallelChunker(max_chunk_size=1000, num_threads=4)
        c.process_directory(self.test_dir)
        
        self.assertGreater(len(c.loaded_files), 90) 

    def test_invalid_encoding_handling(self):
        """Test handling of files with invalid encodings"""
        invalid_file = os.path.join(self.test_dir, "invalid.txt")
        with open(invalid_file, "wb") as f:
            f.write(b"\xff\xfe\x00\x00Invalid UTF")
            
        c = ParallelChunker(max_chunk_size=1000)
        c.process_directory(self.test_dir)

    def test_chunk_file_names(self):
        """Test chunk file naming and numbering"""
        d = os.path.join(self.test_dir, "chunk_names")
        os.mkdir(d)
        c = ParallelChunker(equal_chunks=3, output_dir=d)
        c.process_directory(self.test_dir)
        
        files = sorted(os.listdir(d))
        self.assertEqual(files, ["chunk-0.txt", "chunk-1.txt", "chunk-2.txt"])

if __name__ == "__main__":
    unittest.main()
